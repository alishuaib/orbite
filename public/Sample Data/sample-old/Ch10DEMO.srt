1
00:00:00,000 --> 00:00:01,425
So in this top,

2
00:00:01,425 --> 00:00:04,440
perhaps some decision tree,

3
00:00:04,440 --> 00:00:07,425
we're just continue
working on the same path.

4
00:00:07,425 --> 00:00:09,720
So what I'm going to do,

5
00:00:09,720 --> 00:00:16,335
I store it with
the bagging model.

6
00:00:16,335 --> 00:00:19,830
I've put a backing
on the canvas and

7
00:00:19,830 --> 00:00:25,180
then boosting and
random forest.

8
00:00:25,490 --> 00:00:29,325
Then I connect my
training dataset

9
00:00:29,325 --> 00:00:32,145
to each of the.

10
00:00:32,145 --> 00:00:35,470
I start with the
bagging model.

11
00:00:37,060 --> 00:00:39,605
I can give it a name.

12
00:00:39,605 --> 00:00:41,720
I'll just call
it tagging and

13
00:00:41,720 --> 00:00:44,210
tells me what the
input dataset is.

14
00:00:44,210 --> 00:00:46,460
Again, you can
have a base model.

15
00:00:46,460 --> 00:00:49,565
In this case, I
don't have any.

16
00:00:49,565 --> 00:00:52,760
Then I get to pick my
dependent variable.

17
00:00:52,760 --> 00:00:54,740
And as you can see, I can

18
00:00:54,740 --> 00:00:57,170
pick any types of
dependent variables,

19
00:00:57,170 --> 00:01:00,305
numeric or excrete,
doesn't matter.

20
00:01:00,305 --> 00:01:02,930
In this case, I chose on.

21
00:01:02,930 --> 00:01:05,420
And for independent
variable,

22
00:01:05,420 --> 00:01:07,940
I'm gonna do

23
00:01:07,940 --> 00:01:15,140
a maybe capital gain
or you education.

24
00:01:15,140 --> 00:01:18,785
I use hours per week.

25
00:01:18,785 --> 00:01:23,210
Marital status
and relationships

26
00:01:23,210 --> 00:01:25,580
and check so that

27
00:01:25,580 --> 00:01:28,324
I just few of these
seven variable.

28
00:01:28,324 --> 00:01:31,580
And then I click
next to where

29
00:01:31,580 --> 00:01:34,790
I parameterize
my, my model.

30
00:01:34,790 --> 00:01:38,660
So first thing From
the Top Model,

31
00:01:38,660 --> 00:01:41,000
growth options,
number of trees.

32
00:01:41,000 --> 00:01:45,185
So by default,
tougher have 150.

33
00:01:45,185 --> 00:01:47,960
Please don't elect
to change it

34
00:01:47,960 --> 00:01:50,510
to 50 because it's
going to take a while.

35
00:01:50,510 --> 00:01:53,660
I'm actually going to
fuse 25 if you choose.

36
00:01:53,660 --> 00:01:57,680
250, I'm going to take
five minutes to train.

37
00:01:57,680 --> 00:02:00,440
You can pick a random seed.

38
00:02:00,440 --> 00:02:03,170
For the sampling purpose.

39
00:02:03,170 --> 00:02:05,450
If you don't want
that sampling

40
00:02:05,450 --> 00:02:06,530
with replacement,

41
00:02:06,530 --> 00:02:09,740
you can kind of
have this option.

42
00:02:09,740 --> 00:02:12,275
But I'm not going
to turn that off.

43
00:02:12,275 --> 00:02:14,660
I can also say,

44
00:02:14,660 --> 00:02:17,795
if you want to reduce
your sample size.

45
00:02:17,795 --> 00:02:19,415
So let's say, okay,

46
00:02:19,415 --> 00:02:23,870
give me NH3 maybe
and 50 percent

47
00:02:23,870 --> 00:02:25,775
direct or similar to

48
00:02:25,775 --> 00:02:28,970
diffusion tree when you
grow a decision tree.

49
00:02:28,970 --> 00:02:30,470
Again, so you have

50
00:02:30,470 --> 00:02:33,380
the parameters are
for search method,

51
00:02:33,380 --> 00:02:36,050
I keep cluster, and
then four measures,

52
00:02:36,050 --> 00:02:37,370
I keep entropy value.

53
00:02:37,370 --> 00:02:39,680
And at the bottom you see

54
00:02:39,680 --> 00:02:42,095
the criteria or the
stopping criteria.

55
00:02:42,095 --> 00:02:44,810
When building. Also grown.

56
00:02:44,810 --> 00:02:46,910
It could be trees are going

57
00:02:46,910 --> 00:02:49,205
to be gone automatically.

58
00:02:49,205 --> 00:02:52,355
And I'm not going to
modify any of these.

59
00:02:52,355 --> 00:02:54,905
And I'm just going
to click run.

60
00:02:54,905 --> 00:02:58,205
The model, gets trained.

61
00:02:58,205 --> 00:03:03,305
All right, not bad.
In the result.

62
00:03:03,305 --> 00:03:05,630
When you open your model,

63
00:03:05,630 --> 00:03:08,870
you just feed its
model overview tab.

64
00:03:08,870 --> 00:03:09,890
That tells you what did

65
00:03:09,890 --> 00:03:12,110
your DV and independent

66
00:03:12,110 --> 00:03:15,260
My about yanking
out this time

67
00:03:15,260 --> 00:03:16,580
into an attribute is just

68
00:03:16,580 --> 00:03:18,470
setting of whatever
you had in

69
00:03:18,470 --> 00:03:20,240
the width or your a

70
00:03:20,240 --> 00:03:22,655
and your user
input with whom.

71
00:03:22,655 --> 00:03:24,050
Nothing special.

72
00:03:24,050 --> 00:03:25,545
So you don't
have any effort.

73
00:03:25,545 --> 00:03:27,760
Similar to like let's say,

74
00:03:27,760 --> 00:03:29,305
logistic regression.

75
00:03:29,305 --> 00:03:30,895
You haven't feel a
distribution yet,

76
00:03:30,895 --> 00:03:33,025
but that's a decision tree.

77
00:03:33,025 --> 00:03:35,710
It got kind of
like a black box.

78
00:03:35,710 --> 00:03:38,245
Could either just
so many trees.

79
00:03:38,245 --> 00:03:41,380
We don't have anything
in early done.

80
00:03:41,380 --> 00:03:43,390
What we can do is
dip at the end,

81
00:03:43,390 --> 00:03:46,450
compare the performance
of the model.

82
00:03:46,450 --> 00:03:47,935
The other technique.

83
00:03:47,935 --> 00:03:49,300
Now let's go to the random

84
00:03:49,300 --> 00:03:53,814
forests pain
source of Lindlar.

85
00:03:53,814 --> 00:03:57,085
You just pick a name
for your model.

86
00:03:57,085 --> 00:04:01,420
Then you select your
dependent variable.

87
00:04:01,420 --> 00:04:03,160
And again, I'll
just shoot the

88
00:04:03,160 --> 00:04:06,250
same variable, age, game,

89
00:04:06,250 --> 00:04:09,275
maybe education
hours per week,

90
00:04:09,275 --> 00:04:14,000
marital status and
relationships.

91
00:04:14,000 --> 00:04:18,180
And nine.

92
00:04:18,850 --> 00:04:25,430
And then for an
hour a week, 20 pi,

93
00:04:25,430 --> 00:04:28,685
you get a number
of feature that

94
00:04:28,685 --> 00:04:30,950
the percentage
of independent

95
00:04:30,950 --> 00:04:33,710
liable to use
in each model,

96
00:04:33,710 --> 00:04:35,750
in each tree, in each of

97
00:04:35,750 --> 00:04:40,620
these 25 trees and
shampoo, a 50.

98
00:04:41,230 --> 00:04:44,930
To keep the cluster
method for the,

99
00:04:44,930 --> 00:04:46,880
for the third
method, entropy,

100
00:04:46,880 --> 00:04:48,080
I'm not going to
change anytime you

101
00:04:48,080 --> 00:04:49,505
usually don't
change any integer,

102
00:04:49,505 --> 00:04:51,125
ie, I only worry about

103
00:04:51,125 --> 00:04:54,210
number of tweet and
maybe number of feature.

104
00:04:56,830 --> 00:04:59,315
And then I start

105
00:04:59,315 --> 00:05:03,230
training boosting
tree, boosting model.

106
00:05:03,230 --> 00:05:06,980
Okay, for the most
thing you will see,

107
00:05:06,980 --> 00:05:09,500
I only have access to

108
00:05:09,500 --> 00:05:12,890
binary variables are
dichotomous variable.

109
00:05:12,890 --> 00:05:15,130
That one of the
shortcoming of pollution,

110
00:05:15,130 --> 00:05:18,380
we can only use dichotomous

111
00:05:18,380 --> 00:05:20,090
variable like
response PF and

112
00:05:20,090 --> 00:05:22,115
no fraud or no fraud.

113
00:05:22,115 --> 00:05:26,720
B11, I get to select
my target category.

114
00:05:26,720 --> 00:05:30,630
Again, I use the same
set, our variable.

115
00:05:33,510 --> 00:05:36,895
Hey, man here,

116
00:05:36,895 --> 00:05:38,260
and I'll change the number

117
00:05:38,260 --> 00:05:40,570
of, number of iteration.

118
00:05:40,570 --> 00:05:46,705
We find after shrinkage,
shrinkage factor.

119
00:05:46,705 --> 00:05:48,670
I'm not going to
change that to

120
00:05:48,670 --> 00:05:51,670
keep it at 0.001.

121
00:05:51,670 --> 00:05:53,590
I'm not going to
change anything else.

122
00:05:53,590 --> 00:06:02,560
I just click on. Now
next thing I want to do,

123
00:06:02,560 --> 00:06:04,240
I want to compare

124
00:06:04,240 --> 00:06:06,250
the result of all
these models.

125
00:06:06,250 --> 00:06:07,375
So what I need is

126
00:06:07,375 --> 00:06:12,590
model validation
or of deeds.

127
00:06:12,870 --> 00:06:15,270
I connect them model,

128
00:06:15,270 --> 00:06:20,105
model validation
and connect

129
00:06:20,105 --> 00:06:23,340
all the training data.

130
00:06:30,880 --> 00:06:34,430
And then I go through
the model validation

131
00:06:34,430 --> 00:06:37,955
step flow, agon validation.

132
00:06:37,955 --> 00:06:40,100
I'll go through
the wizard and

133
00:06:40,100 --> 00:06:42,140
not change anything
except Jeff F1 and

134
00:06:42,140 --> 00:06:44,660
a cutoff because I want to

135
00:06:44,660 --> 00:06:47,690
compare these
on-farm models with

136
00:06:47,690 --> 00:06:50,780
my deficient we add and
I change the cutoff

137
00:06:50,780 --> 00:06:54,620
to what I had in for
their decisions,

138
00:06:54,620 --> 00:06:57,485
revalidation chamfer
and change it to

139
00:06:57,485 --> 00:07:00,050
0.73, remove

140
00:07:00,050 --> 00:07:03,260
this additional
column is correct.

141
00:07:03,260 --> 00:07:05,240
I don't care about it.

142
00:07:05,240 --> 00:07:11,430
Then I can expand
this from bagging.

143
00:07:12,070 --> 00:07:15,830
And you see how
validation takes longer.

144
00:07:15,830 --> 00:07:18,155
Have to go through
each record,

145
00:07:18,155 --> 00:07:21,590
have to go to 25
decision tree

146
00:07:21,590 --> 00:07:24,965
pretty today the
probability.

147
00:07:24,965 --> 00:07:27,875
And then they
get average now.

148
00:07:27,875 --> 00:07:31,880
And then the result gets
generated like lift

149
00:07:31,880 --> 00:07:37,025
lower than random forest.

150
00:07:37,025 --> 00:07:46,290
And the cutoff postwar

151
00:07:46,290 --> 00:07:49,150
actually lower because
it has the FAD,

152
00:07:49,150 --> 00:07:52,820
the sham out of
feature selection.

153
00:07:53,280 --> 00:07:58,060
And now boosting
my title one.

154
00:07:58,060 --> 00:08:03,310
I changed the cut-off 0.73,

155
00:08:03,310 --> 00:08:08,575
like click on OK.

156
00:08:08,575 --> 00:08:10,015
Now I take

157
00:08:10,015 --> 00:08:13,790
model analyzer from
evaluate, collect.

158
00:08:16,500 --> 00:08:20,080
All right, so I want to

159
00:08:20,080 --> 00:08:23,055
compare the three models
also with the result

160
00:08:23,055 --> 00:08:25,670
from Interactive be that

161
00:08:25,670 --> 00:08:27,080
I have because I already

162
00:08:27,080 --> 00:08:29,765
have the validated data

163
00:08:29,765 --> 00:08:31,999
that from training data.

164
00:08:31,999 --> 00:08:34,040
If you have the
validated data

165
00:08:34,040 --> 00:08:35,720
that you can also grow,

166
00:08:35,720 --> 00:08:37,055
you use that one.

167
00:08:37,055 --> 00:08:38,990
But if I haven't done that,

168
00:08:38,990 --> 00:08:41,240
then I just used it on.

169
00:08:41,240 --> 00:08:43,385
So this data fact,

170
00:08:43,385 --> 00:08:46,685
if the validated
data from this

171
00:08:46,685 --> 00:08:48,560
interactively grown

172
00:08:48,560 --> 00:08:51,530
efficiently using
the training data,

173
00:08:51,530 --> 00:08:54,290
not the validation Mumbai.

174
00:08:54,290 --> 00:08:55,910
And then I connect

175
00:08:55,910 --> 00:08:59,600
the other dataset to
model and analyze them.

176
00:08:59,600 --> 00:09:02,210
So now input tomato,

177
00:09:02,210 --> 00:09:05,615
analyzer are or data.

178
00:09:05,615 --> 00:09:08,540
Okay, We start
with the first one

179
00:09:08,540 --> 00:09:10,580
which is coming from
the deficiency.

180
00:09:10,580 --> 00:09:13,730
I make sure that I
choose yes probability

181
00:09:13,730 --> 00:09:19,130
but yes category on
yes probability.

182
00:09:19,130 --> 00:09:25,955
And I call this decision
tree interactive.

183
00:09:25,955 --> 00:09:30,035
The next one is from
the bagging I choose.

184
00:09:30,035 --> 00:09:35,135
Yes. And yes probability.

185
00:09:35,135 --> 00:09:39,029
I'll call it bagging.

186
00:09:39,130 --> 00:09:43,430
And random forest painting.

187
00:09:43,430 --> 00:09:45,065
Yes, category

188
00:09:45,065 --> 00:09:48,665
and the farm fields
probability.

189
00:09:48,665 --> 00:09:55,340
And I renamed
the last thing

190
00:09:55,340 --> 00:09:57,575
so we can see that
it can have a

191
00:09:57,575 --> 00:10:00,170
longer if you have
a dataset with

192
00:10:00,170 --> 00:10:03,530
the DV category and

193
00:10:03,530 --> 00:10:07,264
the probability of
that, that PV category.

194
00:10:07,264 --> 00:10:09,635
You can just really
didn't analyze that.

195
00:10:09,635 --> 00:10:12,350
Even if the dataset

196
00:10:12,350 --> 00:10:14,120
is your habit from before,

197
00:10:14,120 --> 00:10:16,400
from somewhere from
different software,

198
00:10:16,400 --> 00:10:17,510
you just need to bring it

199
00:10:17,510 --> 00:10:20,210
in and do the comparison.

200
00:10:20,210 --> 00:10:22,770
And I click Run.

201
00:10:25,510 --> 00:10:28,550
So now, and now you
see that I have for

202
00:10:28,550 --> 00:10:31,565
care the red one and
the decision tree,

203
00:10:31,565 --> 00:10:33,755
which the performance
is lower than.

204
00:10:33,755 --> 00:10:35,225
The other three.

205
00:10:35,225 --> 00:10:36,920
The on-farm model,

206
00:10:36,920 --> 00:10:39,020
the yellow if the boosting,

207
00:10:39,020 --> 00:10:41,480
blue is bagging
and green in

208
00:10:41,480 --> 00:10:44,645
random products like E3
are almost the same.

209
00:10:44,645 --> 00:10:46,655
But you figure out
how they're doing

210
00:10:46,655 --> 00:10:49,220
better than the fish entry?

211
00:10:49,220 --> 00:10:52,370
I look at the
top 40 per fan.

212
00:10:52,370 --> 00:10:58,865
They give me about 80
percent of the target,

213
00:10:58,865 --> 00:11:00,605
85% of the target,

214
00:11:00,605 --> 00:11:03,380
slightly higher than
auto grown three,

215
00:11:03,380 --> 00:11:05,750
I believe it but 83.

216
00:11:05,750 --> 00:11:07,790
Well, here I get 86.

217
00:11:07,790 --> 00:11:10,250
Now I go to lift chart.

218
00:11:10,250 --> 00:11:12,905
You can fit a bill
for the fame.

219
00:11:12,905 --> 00:11:14,570
I should stop using them

220
00:11:14,570 --> 00:11:19,340
or 40 percent of
my population.

221
00:11:19,340 --> 00:11:23,255
And then ROC, I
look at the AU fee.

222
00:11:23,255 --> 00:11:24,650
All right, and you see

223
00:11:24,650 --> 00:11:27,455
for decision tree is 0.83,

224
00:11:27,455 --> 00:11:33,530
while for lead or at
around point ADA,

225
00:11:33,530 --> 00:11:36,860
it bagging 0.89
for random Firth

226
00:11:36,860 --> 00:11:40,310
and 0.88 or boosting.

227
00:11:40,310 --> 00:11:43,025
All right, Now how we

228
00:11:43,025 --> 00:11:47,645
compare performance
of different models.

229
00:11:47,645 --> 00:11:50,554
Mad, I find them useful,

230
00:11:50,554 --> 00:11:53,555
like people use
them in like

231
00:11:53,555 --> 00:11:58,010
customer retention,
churn fraud.

232
00:11:58,010 --> 00:12:00,050
I can go to Action and I

233
00:12:00,050 --> 00:12:02,510
say generate code and let's

234
00:12:02,510 --> 00:12:04,715
say they give me
the Python code

235
00:12:04,715 --> 00:12:07,490
for defy the
only 25 tweets.

236
00:12:07,490 --> 00:12:09,890
But imagine that
you usually start

237
00:12:09,890 --> 00:12:13,130
training for like
100, 150 tweets.

238
00:12:13,130 --> 00:12:15,130
Imagine you need to
look at the rule

239
00:12:15,130 --> 00:12:17,585
for a 150 trees.

240
00:12:17,585 --> 00:12:19,220
If you've just 25 trees,

241
00:12:19,220 --> 00:12:21,440
look at all the fluids and

242
00:12:21,440 --> 00:12:24,065
byte array, guess
co-generation.

243
00:12:24,065 --> 00:12:26,930
We didn't even
have these options

244
00:12:26,930 --> 00:12:28,100
before we just added them

245
00:12:28,100 --> 00:12:29,480
made a couple years ago.

246
00:12:29,480 --> 00:12:32,240
So greeting you to at

247
00:12:32,240 --> 00:12:33,575
least get the code and

248
00:12:33,575 --> 00:12:36,030
put that into production.
