1
00:00:00,000 --> 00:00:04,257
Chapter 9: Model Evaluation and Validation.

2
00:00:04,257 --> 00:00:13,270
Using our models from our previous demonstration in Chapter 8, let’s look at how we can compare and evaluate two models in Knowledge Studio.

3
00:00:13,270 --> 00:00:19,905
First drag a model validation node found under the evaluate palette onto our workflow canvas.

4
00:00:19,905 --> 00:00:25,234
Next, connect our Interactive Model and Training dataset to the node.

5
00:00:25,234 --> 00:00:30,014
Once the node has turned blue, double click to open the wizard.

6
00:00:30,014 --> 00:00:34,115
First, set an appropriate name and click next.

7
00:00:34,115 --> 00:00:43,075
On this second page, you can adjust the fields to make sure they match in case the names are different between the model and dataset being used.

8
00:00:43,075 --> 00:00:50,755
Next, we are shown all the possible validation fields and given the options to adjust our cutoff values.

9
00:00:50,755 --> 00:00:58,618
Let’s change our cutoff values to 0.3 for yes and 0.7 for no then click Next.

10
00:00:58,618 --> 00:01:03,843
On this page, we can choose which fields to include in our output.

11
00:01:03,843 --> 00:01:10,843
In this case, we are only interested in the "Response" and "ID" fields so let’s select those two.

12
00:01:10,843 --> 00:01:16,094
After confirming everything is correct, click run to generate the output.

13
00:01:16,094 --> 00:01:23,147
Once the model validation node outputs the dataset you can double click to view the confusion matrix.

14
00:01:23,147 --> 00:01:27,431
You can see the accuracy for the "No" and "Yes" response here.

15
00:01:27,431 --> 00:01:34,563
Under the Statistics section we can find our total accuracy along with other calculated statistics.

16
00:01:34,563 --> 00:01:39,552
Let’s head back to the workflow canvas and look at our model performance.

17
00:01:39,552 --> 00:01:45,482
First drag the model analyzer node from the evaluate palette onto the canvas.

18
00:01:45,482 --> 00:01:48,930
Next connect our model validation output.

19
00:01:48,930 --> 00:01:51,751
Double click to open the wizard.

20
00:01:51,751 --> 00:01:55,016
Make sure the dependent variable is correct.

21
00:01:55,016 --> 00:01:59,823
Let’s change our predicted value and probability score to Yes.

22
00:01:59,823 --> 00:02:02,827
Give an appropriate name and click run.

23
00:02:02,827 --> 00:02:08,443
Once the status turns green, double click to open the analyzer output.

24
00:02:08,443 --> 00:02:11,787
First take a look at the cumulative lift chart.

25
00:02:11,787 --> 00:02:29,733
This chart compares our model performance to random selection. If we look at the top 40% of our population, we can see that 82% of that population is in the yes category. If we had a "no" model, we would only have 40% by random selection.

26
00:02:29,733 --> 00:02:50,239
Next, let’s look at the Lift Chart. This is a scaled representation of our model’s performance. The straight line represents random selection, so by looking at this we can say that once we hit the top 40% of our population, we should stop using the model as the performance falls below the outcome of random selection.

27
00:02:50,239 --> 00:02:57,214
All the points used to plot these charts can be found under the Cumulative Lift report and Lift Report tabs.

28
00:02:57,214 --> 00:03:12,704
The K-S chart displays a curve showing the cumulative distribution of the dependent variable categories. Hovering over the dotted line, we can tell that at 40% we maximize the separation between No and Yes responses.

29
00:03:12,704 --> 00:03:30,598
The ROC chart compares the True Positive and False Positive rates. At the bottom next to the legend, we can see the AUC and Genie index. The chart is used to interpret how much the False Positive rate needs to be accounted for at a given True Positive rate.

30
00:03:30,598 --> 00:03:38,487
For example, at 80% True Positive rate we would have to account for 25% False Positive rate.

31
00:03:38,487 --> 00:03:51,967
The next tab is a goodness of fit test which uses a variation of chi square test. It’s a test statistic based on the difference between observed and expected frequency of a chi square distribution.

32
00:03:51,967 --> 00:03:57,818
Let’s head back to our workflow canvas and connect our validation dataset as well.

33
00:03:57,818 --> 00:04:02,206
First drag a new model validation node to the canvas.

34
00:04:02,206 --> 00:04:06,856
Next, connect our Interactive Model and Validation dataset.

35
00:04:06,856 --> 00:04:12,917
Let’s go through the wizard and adjust the settings to match our previous model validation node.

36
00:04:12,917 --> 00:04:17,593
Once we have the output, connect it to our model analyzer node.

37
00:04:17,593 --> 00:04:20,414
Double click to open the wizard.

38
00:04:20,414 --> 00:04:26,161
Let’s look at the second tab and change the predicted value and probability score.

39
00:04:26,161 --> 00:04:29,165
Give an appropriate name and click run.

40
00:04:29,165 --> 00:04:33,475
Now double click the analyzer to compare the performance.

41
00:04:33,475 --> 00:04:45,335
As we can see there are no huge gaps (or disparities) between the two lines. Seeing huge gaps generally means the model cannot be used with different datasets.

42
00:04:45,335 --> 00:04:50,455
Let’s head back to the workflow canvas and compare our two models now.

43
00:04:50,455 --> 00:04:58,344
First, add a new model validation node and connect it to the auto grow model alongside the validation dataset.

44
00:04:58,344 --> 00:05:03,568
Open the wizard and set the settings the same to those as previously used.

45
00:05:03,568 --> 00:05:09,576
Next, add a new model analyzer node and connect both validation outputs.

46
00:05:09,576 --> 00:05:12,397
Double click to open the wizard.

47
00:05:12,397 --> 00:05:19,999
Let’s change the predicted value and probability score to Yes for both tabs and give them an appropriate name.

48
00:05:19,999 --> 00:05:23,996
After confirming everything is correct, click run.

49
00:05:23,996 --> 00:05:28,646
Once the status turns green double click to open the report.

50
00:05:28,646 --> 00:05:38,598
Now we can see how both of our models compared with each other. As we can see, our auto grown tree is performing better than our interactive model.

51
00:05:38,598 --> 00:05:45,599
With this we can compare different model performances to identify the best model for our use case.

