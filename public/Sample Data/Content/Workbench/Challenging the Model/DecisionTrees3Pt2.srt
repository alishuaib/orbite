1
00:00:00,000 --> 00:00:10,057
Let’s see what impact a decision forest may have. A Decision Forest block is dragged onto the Workflow canvas and the development partition connected.

2
00:00:10,057 --> 00:00:21,341
Accessing configuration options reveals two pages: Variable Selection and Preferences. The Variable Selection page provides the ability to specify the model.

3
00:00:21,341 --> 00:00:28,133
From the Dependent variable drop-down DV is selected and its treatment is automatically set as Nominal.

4
00:00:28,133 --> 00:00:36,022
The eight variables identified previously with the logistic regression model are moved into the Selected Independent Variables list.

5
00:00:36,022 --> 00:00:43,363
Variable treatment can be modified by selecting from the treatment dropdown and here “education_num” is changed to Interval.

6
00:00:43,363 --> 00:00:54,256
From the Preferences page, model particulars can be set. A decision forest is an ensemble method that builds many decision tree models and combines results.

7
00:00:54,256 --> 00:01:00,107
The number of trees in the decision forest is set to 100 by default but can be modified.

8
00:01:00,107 --> 00:01:12,515
Decision forests score data and add a propensity for each of the dependent variable categories. The resulting propensities are arrived at by summing all the scores from each model and outputting an average.

9
00:01:12,515 --> 00:01:29,077
A decision forest will also create a new variable that classifies observations into one of the dependent variable categories, here either bad or good. Options available to determine how to classify observations are based on Mean Probability or voting, the default being Vote.

10
00:01:29,077 --> 00:01:44,567
Here, the only option that is change is to set the Classifier combination to Mean Probability. Clicking Apply runs the model. Note that as the decision forest is building 100 models the process may take a little longer than normal to run.

11
00:01:44,567 --> 00:01:52,900
Once complete, results are accessed. Again, there are two tabs Decision Forest Results and Scoring code.

12
00:01:52,900 --> 00:02:03,402
The decision forest results are presented in five tables: Model Information, Target Summary, Input Summary, confusion Matrix and Out-Of-Bag Confusion Matrix.

13
00:02:03,402 --> 00:02:12,701
Given that 100 models were developed and results from each of these combined, there is little insight into the model predictions beyond the confusion matrices.

14
00:02:12,701 --> 00:02:25,867
From here it can be seen that the good category is predicted much better than the bad category. The bad category classification rate is slightly in excess of 77%, which on the face of is very good.

15
00:02:25,867 --> 00:02:30,177
The development and testing partitions are scored with the model.

16
00:02:30,177 --> 00:02:49,429
Opening the development partition it can be seen there are three new variables: "P bad" , "P good" and "Predicted Value". This is the classification of each observation into one of the dependent variable categories, based on propensity scores and is created as a result of the Classifier Combination which was set to Mean Probability.

17
00:02:49,429 --> 00:02:59,669
Notice that if an observation has a propensity for bad >= 0.5, then it will be classified as bad, otherwise the observation is classified as good.

18
00:02:59,669 --> 00:03:09,752
Both scored datasets are renamed to ensure they can be easily identified and once complete the scored partitions are connected to the Analyze Models block, configured as before.

19
00:03:09,752 --> 00:03:18,216
Opening results, it can be clearly seen from the Summary that the decision forest is outperforming both the logistic regression and the decision tree.

20
00:03:18,216 --> 00:03:28,717
Bear in mind that a better comparison between the decision forest and the decision tree would be to build the decision tree with the variables identified using the logistic regression and then compare.

21
00:03:28,717 --> 00:03:34,542
From the statistics tab, notice that statistics are highest for the decision forest.

22
00:03:34,542 --> 00:03:55,884
Comparing the decision forest applied to the development and testing partitions, it can be seen that there is a lot of variability across values. Notice the large differences in the C-Statistic for the decision forest applied to the development partition at approximately 0.97 versus approximately 0.92 for the same model applied to the testing partition.

23
00:03:55,884 --> 00:04:07,274
The statistics are much more stable for the other models and given this, the preference in this instance would be for the logistic regression as it outperforms the decision tree and is consistent across partitions.

24
00:04:07,274 --> 00:04:18,010
The decision tree could be further developed to include additional independent variables to better compare with the logistic regression but for now, the logistic regression model is the champion.

