1
00:00:00,000 --> 00:00:15,203
This demonstration uses the project created in a previous lesson and adds a new Workflow named Decision_Tree. The data used in this lesson are contained in the file: Risk_Def.csv,  and is dragged onto the Workflow canvas from the File Explorer view.

2
00:00:15,203 --> 00:00:22,308
Opening the file with the data profiler, it can be seen there are 20000 observations and 13 variables.

3
00:00:22,308 --> 00:00:35,839
Viewing the data, it can be seen there is an ID variable, some demographic, and financial variables, as well as a variable named DV. This will be the focus of the model and used as the dependent variable.

4
00:00:35,839 --> 00:00:44,773
The dependent variable re-cords whether an account has been deemed to be good or bad. Here we will build a model to identify "bad" accounts

5
00:00:44,773 --> 00:01:00,420
It is a good idea to first partition the data. The partition block is dragged from the data preparation group to the "risk _ def" dataset. Notice that two datasets are created immediately, this is due to the block being included in Auto Run.

6
00:01:00,420 --> 00:01:08,832
Opening the configuration dialog, two partitions are created by default with 50% of observations, randomly assigned to each.

7
00:01:08,832 --> 00:01:27,300
More partitions can be added by clicking the add partition button. Partition names are changed to development and testing with a 70      30 split.  A random seed of 1000 is used. The model will be created with the development partition and tested on the testing partition.

8
00:01:27,300 --> 00:01:41,119
Clicking OK creates the partitions. Opening both with the Data Profiler, it can be seen that the development partition contains 14000 observations and the testing partition contains 6000 observations.

9
00:01:41,119 --> 00:01:47,702
From the model training group, a Decision Tree block is dragged onto the development partition, and opened.

10
00:01:47,702 --> 00:01:59,222
Viewing the distribution from univariate charts, it can be seen that it contains two values: bad and good, a breakdown of approximately 24% and 76%, respectively.

11
00:01:59,222 --> 00:02:10,820
The first time the block is opened, the preferences dialog shows automatically. There are three dialog pages: Variable selection, Growth Preferences and Binning Preferences.

12
00:02:10,820 --> 00:02:32,528
From Variable selection, model variables are set. The dependent variable is selected as DV, the tree type automatically changes to classification. Decision trees can be applied to a dependent variable that is either categorical or continuous, if a continuous dependent variable is selected, the tree type would change to regression.

13
00:02:32,528 --> 00:02:46,529
From the Target Category drop-down, "bad" is selected. The model results will be identical regardless of the target category selected. The selection simply impacts the way tree results are presented.

14
00:02:46,529 --> 00:03:02,699
A weight variable can be included. This can be any continuous dataset variable,  one is not included here. All variables but ID are moved from the Unselected Independent Variables list to the Selected Independent Variables list.

15
00:03:02,699 --> 00:03:18,686
All variables appear with an Entropy Variance statistic to guide variable inclusion. Notice the treatment dropdown. Variable type can be set as Nominal, Ordinal or Interval for numeric variables and nominal or ordinal for string variables.

16
00:03:18,686 --> 00:03:26,418
Here, the variable education_num is changed from nominal to interval as it contains the "number of years of education complete."

17
00:03:26,418 --> 00:03:33,941
It is wise to ensure that variable type is set correctly and this facility provides the ability to adjust if necessary.

18
00:03:33,941 --> 00:03:47,055
From Growth Preferences, the algorithm can be chosen. Clicking the Growth algorithm drop-down, it can be seen there are three to choose from:. They include C4.5, CART and BRT.

19
00:03:47,055 --> 00:03:57,243
C4.5 provides options to set the maximum tree depth, and minimum node size, as well as providing options to merge categories, and exclude missing values.

20
00:03:57,243 --> 00:04:03,982
Pruning removes terminal nodes, and subtrees that do not significantly improve predictive accuracy.

21
00:04:03,982 --> 00:04:21,432
CART, similarly, has options to set the maximum tree depth, the minimum node size, exclude missing values and apply pruning. Also, there is a dropdown to select the criterion to determine splits. The default being Gini, but Twoing is also available.

22
00:04:21,432 --> 00:04:39,561
Options for C4.5 and CART are limited. Selecting BRT, which stands for Binary regression tree, provides a range of options that make it the clear choice when building a decision tree. Options available via C4.5 and CART are also present here, bar pruning methods.

23
00:04:39,561 --> 00:04:55,234
Notice the criterion dropdown provides four options: Chi-squared, Entropy Variance, Gini Variance and Information Value, with the default being Information Value. The maximum depth can be set, if not selected, there is no maximum depth.

24
00:04:55,234 --> 00:05:08,322
The options: Select minimum node size by ratio, Minimum node size (%) and Minimum node size relate to the minimum node creation size, this can be set as a % of the overall dataset size or as a value.

25
00:05:08,322 --> 00:05:15,688
Tree nodes will only be created if they reach a default threshold size, here .5% of the overall dataset size.

26
00:05:15,688 --> 00:05:27,025
The dataset here contains 14000 observations. Therefore, a half a percent, which equates to 700 is the minimum size a node must be, to be created.

27
00:05:27,025 --> 00:05:36,273
The next three options are similar, but relate to the size a tree node must be to be split further, currently this is set at 2%, but can be changed.

28
00:05:36,273 --> 00:05:41,758
Options selected by default and not present in C4.5 or CART, are:

29
00:05:41,758 --> 00:05:44,945
• Allow same variable split; and

30
00:05:44,945 --> 00:05:47,636
• Open left and open right

31
00:05:47,636 --> 00:06:12,086
The same variable split means that a variable may be split further.  For example, if a split was created with the variable and two nodes created age: <=30 and greater than 30, selecting this option means that either category may undergo further splitting. For example, the greater than 30 category might be split into two categories: 30 - 41 and 42+ in a subsequent split.

32
00:06:12,086 --> 00:06:41,160
Open left and Open right ensure that values below a dataset minimum or above a dataset maximum, for any variable, will be accommodated. For example, If a model uses the variable age and its range is 20 - 89. Let's say the first bin for this variable is 20 - 30. This excludes all values below 20, using open left, the bin range changes to <=30, this is a useful option to include and here, is selected by default.

33
00:06:41,160 --> 00:06:58,375
Missing values are included by default in the tree, but will always appear as a separate category. They can be excluded by selecting "Exclude missing values" or can be forced to be a valid category by selecting merge missing bin and will, therefore, be considered for merging with other bins.

34
00:06:58,375 --> 00:07:14,623
Monotonicity can be forced using weight of evidence. Also, the number of bins can be set, including the maximum number of optimal bins. Other options relate to the criterion chosen, and thresholds for selecting and splitting variables.

35
00:07:14,623 --> 00:07:28,912
At this point, all options are accepted at their defaults.  Help is available from within the dialog by clicking the help icon. Selecting Decision Tree Preferences opens the help pages for all growth preference options.

36
00:07:28,912 --> 00:07:50,385
The third page relates to binning. Workbench decision trees provides automatic and interactive growing methods with the ability to force variables into a model. Given this, the selected algorithm can be used to choose variables and decide splits. Or variables can be forced into the model and splits applied by way of binning.

37
00:07:50,385 --> 00:08:06,372
This page contains two areas: Default bin count, and Winsorrate and Optimal Binning. The Default bin count and Winsorrate relate to Equal Height, Equal Width and Winsorized binning. The Optimal Binning area relates to optimal binning settings.

38
00:08:06,372 --> 00:08:18,336
Setting can be changed, for example the default bin count can be increased to give greater granularity across splits. This page will be revisited when forcing variables into the model.

