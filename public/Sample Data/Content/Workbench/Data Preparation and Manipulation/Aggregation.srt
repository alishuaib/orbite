1
00:00:00,000 --> 00:00:09,612
To prepare the data, it may be useful to group observations by ID,  and generate some summaries - such as average and total for number of products and total price.

2
00:00:09,612 --> 00:00:19,043
The Aggregate block can be used for this purpose, the block is dragged from the Data Preparation group to the Workflow canvas and connected to the merged dataset.

3
00:00:19,043 --> 00:00:28,603
As with all newly connected blocks, the configuration has not been set and the status indicator message: reads "No function provided",  which reflects this.

4
00:00:28,603 --> 00:00:48,509
Double-clicking opens the block configuration dialog. The dialog has two pages. Grouping Variable Selection allows selection of one or more grouping variable, here ID is double clicked and automatically moved to the Selected Grouping Variables list. Observations will now be grouped by ID.

5
00:00:48,509 --> 00:01:05,906
From the expressions page, new variables can be created based on aggregate functions. Options exist to 1 - specify the variable to base the calculation on, and 2- the function to apply. We also need to specify the name to assign to the calculated variable.

6
00:01:05,906 --> 00:01:13,821
Here, there is interest in creating new variables that calculate the sum and average number of products purchased, and total price.

7
00:01:13,821 --> 00:01:31,114
To create average number of products, "num_products" is selected from the variable dropdown. The function dropdown provides a long list of possible options. Here average is selected, notice the new variable name is added automatically but can be changed if desired.

8
00:01:31,114 --> 00:01:47,728
Expressions can be added or deleted by selecting the appropriate icon. Clicking the "Add" expression adds a new expression entry. The same variable is chosen and this time sum is selected and the same two calculations are repeated for the variable tot_price.

9
00:01:47,728 --> 00:01:53,815
OK is clicked to complete the process and the resulting dataset is opened with the Data Profiler.

10
00:01:53,815 --> 00:02:06,667
As can be seen there are 239 observations and 5 variables. From the Data View, it can be seen that transactional information has been summarized and there is now one row for each unique id.

11
00:02:06,667 --> 00:02:15,235
The next step is to enrich the transactional data with the demographic details, contained in the file demographics_ w p s .csv.

12
00:02:15,235 --> 00:02:35,872
First, the file is dragged onto the Workflow canvas from the File Explorer, configured and imported. Opening the file with the Data Profiler, there are 20000 observations and 8 variables. Viewing the data, it can be seen it contains an id variable and this will be used to join demographic details to the transactional data.

13
00:02:35,872 --> 00:02:57,370
The join block will be used for this but prior to proceeding it is wise to ensure there are no duplicates contained in the demographics file. To do this, a Sort block is added and docked with the demographicâ€™s dataset. The sort block can be used to sort data based on one or more variables, here, ID is selected and the option: Remove duplicate keys is chosen.

14
00:02:57,370 --> 00:03:11,999
Opening the results with the Data Profiler, and from the Summary View tab , it can be seen there are 20000 observations, no change from the previous file. Regardless of this file having no duplicates, it is good practice to check for them.

15
00:03:11,999 --> 00:03:21,769
The final step is to enrich the transactional data with demographics. At this point the file names are changed to transactions and demographics.

16
00:03:21,769 --> 00:03:37,939
To complete the process, a Join block is dragged from the Data Preparation group to the Workflow canvas. As always, the configuration settings indicator is red and displays two messages: At least two connections being required and At least one join must be defined.

17
00:03:37,939 --> 00:03:46,925
Both datasets are connected to the Join block and its configuration dialog accessed, where both files are illustrated listing variables in both.

18
00:03:46,925 --> 00:03:54,840
Here there is interest in retaining all observations in the transactions file and enriching with demographics based on the variable id.

19
00:03:54,840 --> 00:04:13,047
The join variable, ID, is dragged from transactions to ID in demographics. A join is created and can be defined by either right-clicking the line joining the variables and selecting the option: Configure Join or simply by double-clicking. Either will open the Join Properties dialog.

20
00:04:13,047 --> 00:04:20,100
The left dataset is transactions and the right dataset is demographics, with ID as the column for both.

21
00:04:20,100 --> 00:04:34,546
There are six join types to choose from, and also six join operators. To retain all observations from transactions, and matching observations from demographics the Join Type is Left Outer and the Join operator selected as equals.

22
00:04:34,546 --> 00:04:45,621
There are also two additional tabs in the Join block: SQL, and Preview. The SQL tab outputs the code for the join, and Preview illustrates what results will look like.

23
00:04:45,621 --> 00:05:06,702
Clicking CTRL+S updates settings, and applies the join. Returning to the Workflow and opening the resulting dataset with the Data Profiler, it can be seen that there are 239 observations, and summarised transactional information has been enriched with demographics. The data can be viewed in greater detail from the Data tab.

